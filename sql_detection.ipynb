{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import numpy\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(query):\n",
    "    tokens=query.upper()\n",
    "    if \"WHERE\" in tokens:\n",
    "        tokens=tokens.split(\"WHERE\",1)\n",
    "        tokens=tokens[1]\n",
    "    Dict = { '!=': 'NEQ', '<>': 'NEQ', '&&': 'AND', '||': 'OR', '/*': ' CMTST ',\n",
    "            '*/': 'CMTEND',\n",
    "            '~': 'TLDE', '!': 'EXCLM', '@': 'ATR', '#': 'HASH', '$': 'DLLR', '%': 'PRCNT',\n",
    "            '^': 'XOR', '&': 'BITAND','|': 'BITOR', '*': 'STAR', '--': 'CMNT', '-': 'MINUS',\n",
    "            '+': 'PLUS', '=': 'EQ', '/': 'SLSH', '?': 'QSTN','.': 'DOT',\n",
    "            ',': 'CMMA', '>': 'GT', '<': 'LT', '\\'': 'SQUT', '\"': 'DQUT', ';': 'SMCLN', \n",
    "            ':': 'CLN', '\\\\': 'BSLSH',\n",
    "            ']': 'RSQBR', '[': 'LSQBR', '}': 'RCBR', '{': 'LCBR', ')': 'RPRN', '(': 'LPRN'}\n",
    "    i=0 #counter\n",
    "    while i < len(tokens):\n",
    "        if tokens[i] in Dict: #if token is in dict\n",
    "            if i + 1 < len(tokens) and (tokens[i] + tokens[i + 1]) in Dict: #if two things after each other replace such as ||\n",
    "                tokens=tokens.replace((tokens[i] + tokens[i + 1]),\" \"+Dict[(tokens[i] + tokens[i + 1])]+\" \")\n",
    "            else:\n",
    "                tokens=tokens.replace(tokens[i], \" \"+Dict[tokens[i]]+\" \")# if only 1 replace it\n",
    "           \n",
    "        i = i + 1 #increment counter\n",
    "  \n",
    "    tokens=word_tokenize(tokens) #nltk function to split string into tokens\n",
    "    i=0\n",
    "    while i < len(tokens):\n",
    "        if re.search(\"^((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\\\.){3}(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$\", tokens[i]):\n",
    "            tokens[i] = \"IPADDR\"\n",
    "        elif re.search(\"^([0-9]+)$\", tokens[i]):\n",
    "            tokens[i] = \"INT\"\n",
    "        elif re.search(\"^([A-Fa-f0-9]{2,})$\", tokens[i]):\n",
    "            tokens[i] = \"HEX\"\n",
    "        elif re.search(\"^\\d*[.,]?\\d*$\", tokens[i]):\n",
    "            tokens[i] = \"DEC\"\n",
    "        elif re.search(\"^([A-Za-z])$\", tokens[i]):\n",
    "            tokens[i] = \"CHR\"\n",
    "        else:\n",
    "            if i-1 > 0 and tokens[i-1] == \"FROM\":\n",
    "                tokens[i]= \"TABLE\"\n",
    "        i = i + 1 #increment counter        \n",
    "    return \" \".join(tokens) # convert it to string again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = open(\"/home/mariam/Desktop/dataset/queries.txt\", \"rt\")\n",
    "tokenized = open(\"/home/mariam/Desktop/dataset/tokenized.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "while True:\n",
    "    counter=counter+1\n",
    "    line=queries.readline() #take the query from queries file\n",
    "    if not line: #if there is no query stop\n",
    "        break\n",
    "    line.strip() \n",
    "    line=line.split(\" \",1) # to split the prefix label\n",
    "    query=tokenization(line[1]) # call tokenization function\n",
    "    tokenized.writelines([line[0]+\" \"+query,\"\\n\"]) # write tokenized string in the file called \"tokenized\"\n",
    "queries.close()\n",
    "tokenized.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training=open(\"/home/mariam/Desktop/dataset/train.txt\", \"w\") #450 malicious #407 non-malicious #training\n",
    "test=open(\"/home/mariam/Desktop/dataset/test.txt\", \"w\") # 150 malicious #100 non-malicios #testing\n",
    "tokenized = open(\"/home/mariam/Desktop/dataset/tokenized.txt\", \"r\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "while True:\n",
    "    counter=counter+1\n",
    "    line=tokenized.readline() #read tokenized query\n",
    "    if not line:\n",
    "        break\n",
    "    line.strip()\n",
    "    if counter<=407: #fill 407 non malicious in training\n",
    "        training.writelines([line,\"\\n\"])\n",
    "    elif counter>407 and counter<=507: #fill 100 non malicious in valid\n",
    "        test.writelines([line,\"\\n\"])\n",
    "    if counter>507 and counter<=957: #fill 450 malicious in training\n",
    "        training.writelines([line,\"\\n\"])\n",
    "    elif counter>957 and counter<=1107: #fill 150 non malicious in valid\n",
    "        test.writelines([line,\"\\n\"])\n",
    "\n",
    "        \n",
    "        \n",
    "test.close()    \n",
    "tokenized.close()\n",
    "training.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised('/home/mariam/Desktop/dataset/train.txt') #train the model using training file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.0%\n"
     ]
    }
   ],
   "source": [
    "#print(model.test('/home/mariam/Desktop/dataset/valid.txt'))\n",
    "total, precision, recall = model.test('/home/mariam/Desktop/dataset/test.txt') #test the model\n",
    "Fmeasure= 2 * ((precision*recall)/(precision + recall)) #calc f-measure\n",
    "print(str(Fmeasure * 100) + \"%\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__label__nonmalicious'] \n",
      "\n",
      "['__label__nonmalicious'] \n",
      "\n",
      "['__label__nonmalicious'] \n",
      "\n",
      "['__label__nonmalicious'] \n",
      "\n",
      "['__label__nonmalicious'] \n",
      "\n",
      "['__label__malicious'] \n",
      "\n",
      "['__label__malicious'] \n",
      "\n",
      "['__label__malicious'] \n",
      "\n",
      "['__label__malicious'] \n",
      "\n",
      "['__label__malicious'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "texts = ['CUSTOMERNAME LIKE SQUT _R PRCNT SQUT SMCLN',\n",
    "        'UPDATE USERS SET EMAIL EQ LOWER LPRN EMAIL RPRN SMCLN',\n",
    "        'DROP TABLE IF EXISTS CUSTOMER CASCADE SMCLN',\n",
    "        'COUNTRY IN LPRN SQUT USA SQUT CMMA SQUT UK SQUT CMMA SQUT JAPAN SQUT RPRN',\n",
    "        'INSERT INTO CUSTOMER LPRN NAME RPRN SELECT RANDOM LPRN RPRN CLN CLN TEXT FROM TABLE LPRN INT CMMA INT RPRN SMCLN',\n",
    "        'INT PRCNT DQUT RPRN RPRN RPRN UNION ALL SELECT NULL CMMA NULL CMMA NULL CMMA NULL CMMA NULL CMMA NULL CMMA NULL CMMA NULL CMNT',\n",
    "        'INT EQ INT AND LPRN SELECT STAR FROM TABLE SELECT LPRN SLEEP LPRN INT RPRN RPRN RPRN FZNO RPRN CMNT',\n",
    "        'ID EQ INT OR LPRN BSLSH DOT RPRN EQ INT UNION SELECT INT CMMA ATR ATR VERSION CMNT INT',\n",
    "        'DECLARE ATR CHR NVARCHAR LPRN INT RPRN 0X730065006C00650063007400200040004000760065007200730069006F006E00 EXEC LPRN ATR CHR RPRN',\n",
    "        'OR CHR EQ CHR']\n",
    "    \n",
    "labels = model.predict(texts)\n",
    "for i in labels[0]:\n",
    "    print (i , \"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID EQ INT PLUS DLLR DOT OR INT EQ INT CMNT INT\n"
     ]
    }
   ],
   "source": [
    "print(tokenization('select * from users where id = 1 +$ . or 1 = 1 -- 1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__malicious',), array([0.98918331]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict('ID EQ INT PLUS DLLR DOT OR INT EQ INT CMNT INT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"THANK YOU\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
